# LectureNote3: CS221 (FALL2021, Stanford)

## Stochastic Gradient Descent (SGD)
- gradient descent is slow
  - each iteration requires going over "all training examples" $\sum_{(x,y) \in D_{train}} Loss(x, y, w) \leftarrow$ expensive when have lots of data
<br>
- Algorithm: Stochastic Gradient Descent
  - initialize w = [0, ... ,0]
  - for t = 1, ... , T:
    - for $(x, y) \in D_{train}$
      - $w \leftarrow w - \eta \nabla_w Loss(x, y, w)$
  - instead of going through all training examples and performing **one** update, perform an update "after each example" $\rightarrow$ much faster
  - trade off: each update is not high quality
    - It's not about *quality*, it's about **quantity**.
  - stochastic gradient descent can escape local minima more effectively compared to batch gradient descent
    - it introduces noise into the optimization process by using a random subset of data to estimate the gradient, and the noise allows SGD to have a higher chance of escaping shallow local minima
---
- Question: What should $\eta$ be?
  - small $\eta$
    - pros: conservative, more stable
    - cons: slow, might get stuck in local minima
  - big $\eta$ 
    - pros: aggressive, faster
    - cons: might overshoot the minimum, leading to divergence or oscillation
- Strategies:
  - constant: $\eta$ = 0.1
  - decreasing: $\eta$ = $\frac{1}{\sqrt{number \ of \ updates \ made \ so \ far}}$
---

## 