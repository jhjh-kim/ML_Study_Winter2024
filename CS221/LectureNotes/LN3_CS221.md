# LectureNote3: CS221 (FALL2021, Stanford)

## More About Optimization
### Stochastic Gradient Descent (SGD)
- gradient descent is slow
  - each iteration requires going over "all training examples" $\sum_{(x,y) \in D_{train}} Loss(x, y, w) \leftarrow$ expensive when have lots of data
<br>
- Algorithm: Stochastic Gradient Descent
  - initialize w = [0, ... ,0]
  - for t = 1, ... , T:
    - for $(x, y) \in D_{train}$
      - $w \leftarrow w - \eta \nabla_w Loss(x, y, w)$
  - instead of going through all training examples and performing **one** update, perform an update "after each example" $\rightarrow$ much faster
  - trade off: each update is not high quality
    - It's not about *quality*, it's about **quantity**.
  - stochastic gradient descent can escape local minima more effectively compared to batch gradient descent
    - it introduces noise into the optimization process by using a random subset of data to estimate the gradient, and the noise allows SGD to have a higher chance of escaping shallow local minima
---
- Question: What should $\eta$ be?
  - small $\eta$
    - pros: conservative, more stable
    - cons: slow, might get stuck in local minima
  - big $\eta$ 
    - pros: aggressive, faster
    - cons: might overshoot the minimum, leading to divergence or oscillation
- Strategies:
  - constant: $\eta$ = 0.1
  - decreasing: $\eta$ = $\frac{1}{\sqrt{number \ of \ updates \ made \ so \ far}}$
---

### Group DRO
- Inequalities between different groups arise in machine learning because the goal of optimization is to minize the **average loss**
<br>
- Linear regression with groups
$\begin{matrix} x & y & group \\ 1 & 4 & A \\ 2 & 8 & A \\ 5 & 5 & B \\ 6 & 6 & B \\ 7 & 7 & B \\ 8 & 8 & B \\ \end{matrix}$
  - predictor $f_w(x) = w \cdot \phi(x)$ does not use the group information
  - Neither $TrainLoss(w)$ nor $Loss(w)$ uses group information
<br>
- Per-group loss
  - $TrainLoss_g(w) = \frac{1}{|D_{train}(g)|}\sum_{(x,y) \in D_{train}(g)}Loss(x,y,w)$
  - Disparity in loss between different groups
  - For w = 1
    - $TrainLoss_A(1) = 22.5$
    - $TrainLoss_B(1) = 0$
<br>
- Maximum group loss
  - $TrainLoss_{max}(w) = max_g \ TrainLoss_g(w)$
  - $TrainLoss_{max}(w) = max(22.5, 0) = 22.5$
<br>
- Average group loss vs Maximum group loss
  ![Alt text](image.png)
  ![Alt text](image-2.png)
  - Standard learning: minimizer of average loss $w = 1.09$
    - linear regression can result in a biased line that tilts towards a larger size group
  - Group distributionally robust optimization (group DRO): minimizer of maximum group loss $w = 1.58$
    - treats groups more equally regardless of the size of each group

- Training via gradient descent 